from fastapi import Depends, FastAPI, HTTPException, status, Header, Body
from fastapi.middleware.cors import CORSMiddleware
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import time

#ML Imports
from transformers import pipeline
import google.generativeai as genai
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

#Imports from my files
from scrapingbsf import make_data
from scraping_selenium import people_also_ask
from schemas import inputRequest

app = FastAPI(title="IPD Back-End",)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

genai.configure(api_key="AIzaSyAYwK3xLs3CoOevA29JgDUuMCx_rGQQIgA")
LLM_model = genai.GenerativeModel('LLM-pro')

from mlx_lm import load, generate

model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.3")

@app.get("/")
def read_root():
    return {"You're not": "supposed to be here"}
#-------------------------------------------------------------------------------------------------------------------------------------
@app.post("/FactCC-QnA")
async def quick_search(request: inputRequest):
    quickSearchAnswer = google_search(request.input)

    #compare scraped info as source and headline as the claim
    ans = pipe([[[quickSearchAnswer,request.input]]], truncation=True, padding='max_length')

    # Display the result
    if ans[0]['label'] == 'INCORRECT':
        ans[0]['score'] = 1 - ans[0]['score']
    
    classification_result = {
        "label": ans[0]['label'],
        "score": ans[0]['score']
    }
    return {'scrapedContent':quickSearchAnswer,'result':classification_result}
#-------------------------------------------------------------------------------------------------------------------------------------
#To answer the question generated by LLM via google
def google_search(query):
    headers = {
        'User-agent':
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'
    }
    
    # Perform the Google search
    search_url = f'https://www.google.com/search?q={query}'
    html = requests.get(search_url, headers=headers)
    
    # Parse the HTML response
    soup = BeautifulSoup(html.text, 'html.parser')
    
    #extract answer from summary answer on google
    if soup.select_one('.hgKElc'):
        answer = soup.select_one('.hgKElc').text 

    # Extract the answer (assuming it's in a specific class) -> QUICK ANSWER BOX
    elif soup.select_one('.DI6Ufb'):
        answer = soup.select_one('.DI6Ufb')
        answer = answer.find(class_='Z0LcW t2b5Cf').text
        answer=query+' '+answer
    
    elif not soup.select_one('.DI6Ufb'):
        result=people_also_ask(search_url)
        if len(result)==2:
            answer=f'{result[0]} {result[1]}'
        else:
            answer = 'Could not retrieve articles related to headline, Could possibly be a false claim.'
    
    return answer

@app.post("/FactCC-LLM")
async def LLM(request: inputRequest):
    
    input = f'''<\s>[INST] You're tasked with fact-checking news headlines for accuracy. 
    Given a headline, generate 1 question that needs to be true to verify the 
    headlines authenticity using a Google search to scrape the answer from the quick search box. 
    Ask the crucial questions first. Your output should only be one question in string for me to ingest in my backend without any other text.
    The headline is : {request.input}< [/INST] '''
    response = generate(model, tokenizer, prompt=input)

    quickSearchAnswer = google_search(response)
    
    #compare scraped info as source and headline as the claim
    ans = pipe([[[quickSearchAnswer,request.input]]], truncation=True, padding='max_length')

    # Display the result
    if ans[0]['label'] == 'INCORRECT':
        ans[0]['score'] = 1 - ans[0]['score']
    
    classification_result = {
        "label": ans[0]['label'],
        "score": ans[0]['score']
    }
    return {"generatedLLMQuestion": response,'scrapedContent':quickSearchAnswer,'result':classification_result}

#-------------------------------------------------------------------------------------------------------------------------------------

#To scrape headlines for the FactCC endpoint
def dataframegen(text_input):
    scraped_df = make_data(text_input)
    scraped_df.dropna(inplace=True)
    return scraped_df

pipe = pipeline(model="manueldeprada/FactCC", task="text-classification", max_length=512)

@app.post("/FactCC-articles")
async def factCC(request: inputRequest):
    scraped_df =  dataframegen(request.input)

    #To make sure best scraped article picked
    scraped_df = scraped_df.sort_values(by='Content', key=lambda x: x.str.len(), ascending=False) 

    if len(scraped_df) == 0 or not scraped_df['Title'][0] or '403 Forbidden' in scraped_df['Content'][0] or '403 Forbidden' in scraped_df['Title'][0] :
        raise HTTPException(status_code=404, detail="Could not retrieve articles related to headline, Could possibly be a false claim.")

    scraped_content = (
    f"{scraped_df['Title'][0]} \n{scraped_df['Content'][0]}")
    
    # Perform text classification [source,claim]
    ans = pipe([[[scraped_content,request.input]]], truncation=True, padding='max_length')

    # Display the result
    if ans[0]['label'] == 'INCORRECT':
        ans[0]['score'] = 1 - ans[0]['score']
    
    classification_result = {
        "label": ans[0]['label'],
        "score": ans[0]['score']
    }
    return {'article':scraped_content,'result':classification_result}

#-------------------------------------------------------------------------------------------------------------------------------------
def perform_text_classification(scraped_content, claim):
    # Perform text classification [source, claim]
    ans = pipe([[[scraped_content, claim]]], truncation=True, padding='max_length')

    # Display the result
    if ans[0]['label'] == 'INCORRECT':
        ans[0]['score'] = 1 - ans[0]['score']

    classification_result = {
        "label": ans[0]['label'],
        "score": ans[0]['score']
    }
    return classification_result

def aggregate_results(LLM_result, QNA_result, Article_result):
    # Aggregate results and return majority-voted label
    LLM_label = LLM_result["label"]
    qna_label = QNA_result["label"]
    article_label = Article_result["label"]

    # Count votes
    votes = {"CORRECT": 0, "INCORRECT": 0}
    for label in [LLM_label, qna_label, article_label]:
        if label == "CORRECT":
            votes["CORRECT"] += 1
        elif label == "INCORRECT":
            votes["INCORRECT"] += 1

    # Determine final result based on majority voting
    if votes["CORRECT"] > votes["INCORRECT"]:
        final_label = "CORRECT"
    elif votes["CORRECT"] < votes["INCORRECT"]:
        final_label = "INCORRECT"
    else:
        final_label = "Cannot draw conclusion (TIE)"

    return {'label':final_label,"votes":votes}

# Define a function for each pipeline
def articles_pipeline(request):
    scraped_df =  dataframegen(request.input)
    scraped_df = scraped_df.sort_values(by='Content', key=lambda x: x.str.len(), ascending=False)
    if (len(scraped_df) == 0) or (not scraped_df['Title'][0]) or ('403 Forbidden' in scraped_df['Content'][0]) or ('403 Forbidden' in scraped_df['Title'][0]) or (len(scraped_df['Content'][0])<400):
        scraped_article="Could not retrieve articles related to headline, Could possibly be a false claim."
        Article_result = {"label": "NA", "score": 0}
    else:
        scraped_article = (f"{scraped_df['Title'][0]} \n{scraped_df['Content'][0][:500]}")
        Article_result = perform_text_classification(scraped_article, request.input)
    return scraped_article, Article_result

def LLM_pipeline(request):

    input = f'''<\s>[INST] You're tasked with fact-checking news headlines for accuracy. 
    Given a headline, generate 1 question that needs to be true to verify the 
    headlines authenticity using a Google search to scrape the answer from the quick search box. 
    Ask the crucial questions first. Your output should only be one question in string for me to ingest in my backend without any other text.
    The headline is : {request.input}< [/INST] '''
    LLM_question = generate(model, tokenizer, prompt=input)
    
    LLMSearchAnswer = google_search(LLM_question)
    LLM_result = perform_text_classification(LLMSearchAnswer, request.input)
    return LLM_question, LLMSearchAnswer, LLM_result

def qna_pipeline(request):
    QNASearchAnswer = google_search(request.input)
    QNA_result = perform_text_classification(QNASearchAnswer, request.input)
    return QNASearchAnswer, QNA_result

# Execute the pipelines concurrently
def execute_pipelines(request):
    with ThreadPoolExecutor() as executor:
        # Submit each pipeline function to the executor
        future_articles = executor.submit(articles_pipeline, request)
        future_LLM = executor.submit(LLM_pipeline, request)
        future_qna = executor.submit(qna_pipeline, request)

        # Retrieve results when all tasks are completed
        scraped_article, Article_result = future_articles.result()
        LLM_question_text, LLMSearchAnswer, LLM_result = future_LLM.result()
        QNASearchAnswer, QNA_result = future_qna.result()

    return scraped_article, Article_result, LLM_question_text, LLMSearchAnswer, LLM_result, QNASearchAnswer, QNA_result

# Define your FastAPI endpoint
@app.post("/FactCC-combined")
async def combined_factCC(request: inputRequest):
    start_time = time.time()
    # Execute pipelines concurrently
    scraped_article, Article_result, LLM_question_text, LLMSearchAnswer, LLM_result, QNASearchAnswer, QNA_result = execute_pipelines(request)

    # Aggregate results
    final_result = aggregate_results(LLM_result, QNA_result, Article_result)
    end_time = time.time()
    time_taken = end_time - start_time
    return {
        "FactCCLLM": {
            "generatedLLMQuestion": LLM_question_text,
            "scrapedContent": LLMSearchAnswer,
            "result": LLM_result
        },
        "FactCCqna": {
            "scrapedContent": QNASearchAnswer,
            "result": QNA_result
        },
        "FactCCarticles": {
            "scrapedContent": scraped_article,
            "result": Article_result
        },
        "FinalResult": final_result,
        "Time Taken" : time_taken
    }

#-------------------------------------------------------------------------------------------------------------------------------------
tokenizer_Bert_Binary_FakeReal = AutoTokenizer.from_pretrained(
    "Arjun24420/BERT-FakeOrReal-BinaryClassification")
model_Bert_Binary_FakeReal = AutoModelForSequenceClassification.from_pretrained(
    "Arjun24420/BERT-FakeOrReal-BinaryClassification")

# Define class labels mapping
class_mapping_Bert_Binary_FakeReal = {
    1: 'Reliable',
    0: 'Unreliable',
}

@app.post("/BertBinaryfr")
def predict_Bert_Binary_FakeOrReal(request: inputRequest):
    # Tokenize the input text
    inputs = tokenizer_Bert_Binary_FakeReal(request.input, padding=True, truncation=True,
                       max_length=512, return_tensors="pt")

    # Get model output (logits)
    outputs = model_Bert_Binary_FakeReal(**inputs)

    # Calculate probabilities
    probs = outputs.logits.softmax(1)

    # Get the probabilities for each class
    class_probabilities = {class_mapping_Bert_Binary_FakeReal[i]: probs[0, i].item()
                           for i in range(probs.shape[1])}

    return class_probabilities
#-------------------------------------------------------------------------------------------------------------------------------------