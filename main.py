from fastapi import Depends, FastAPI, HTTPException, status, Header, Body
from fastapi.middleware.cors import CORSMiddleware
import requests
from bs4 import BeautifulSoup

#ML Imports
from transformers import pipeline
import google.generativeai as genai
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

#Imports from my files
from scrapingbsf import make_data
from scraping_selenium import people_also_ask
from schemas import inputRequest

app = FastAPI(title="IPD Back-End",)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

genai.configure(api_key="AIzaSyAYwK3xLs3CoOevA29JgDUuMCx_rGQQIgA")
gemini_model = genai.GenerativeModel('gemini-pro')

@app.get("/")
def read_root():
    return {"You're not": "supposed to be here"}

#-------------------------------------------------------------------------------------------------------------------------------------
#To answer the question generated by gemini via google
def google_search(query):
    headers = {
        'User-agent':
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'
    }
    
    # Perform the Google search
    search_url = f'https://www.google.com/search?q={query}'
    html = requests.get(search_url, headers=headers)
    
    # Parse the HTML response
    soup = BeautifulSoup(html.text, 'html.parser')
    
    # Extract the answer (assuming it's in a specific class)
    if soup.select_one('.DI6Ufb'):
        answer = soup.select_one('.DI6Ufb').text
    
    elif not soup.select_one('.DI6Ufb'):
        result=people_also_ask(search_url)
        if len(result)==2:
            answer=result[0]+'\n'+result[1]
        else:
            answer = 'Could not retrieve articles related to headline, Could possibly be a false claim.'
    
    return answer

@app.post("/gemini-FactCC")
async def gemini(request: inputRequest):
    
    response = gemini_model.generate_content(
        f'''You're tasked with fact-checking news headlines for accuracy. Given a headline, generate 1 questions that needs to be true to verify the headlines authenticity using a Google search to scrape the answer from the quick search box. 
            Ask the crucial questions first. Your output should only be in the form of a string for me to ingest in my backend without any other text.
            The headline is : {request.input}'''
    )

    quickSearchAnswer = google_search(response.text)
    
    #compare scraped info as source and headline as the claim
    ans = pipe([[[quickSearchAnswer,request.input]]], truncation=True, padding='max_length')

    # Display the result
    if ans[0]['label'] == 'INCORRECT':
        ans[0]['score'] = 1 - ans[0]['score']
    
    classification_result = {
        "label": ans[0]['label'],
        "score": ans[0]['score']
    }
    return {"Question": response.text,'Answer':quickSearchAnswer,'Result':classification_result}

#-------------------------------------------------------------------------------------------------------------------------------------
#To scrape headlines for the FactCC endpoint
def dataframegen(text_input):
    scraped_df = make_data(text_input)
    scraped_df.dropna(inplace=True)
    return scraped_df

pipe = pipeline(model="manueldeprada/FactCC", task="text-classification", max_length=512)

@app.post("/FactCC")
async def factCC(request: inputRequest):
    scraped_df =  dataframegen(request.input)

    #To make sure best scraped article picked
    scraped_df = scraped_df.sort_values(by='Content', key=lambda x: x.str.len(), ascending=False) 

    if len(scraped_df) == 0 or not scraped_df['Title'][0] or '403 Forbidden' in scraped_df['Content'][0] or '403 Forbidden' in scraped_df['Title'][0] :
        raise HTTPException(status_code=404, detail="Could not retrieve articles related to headline, Could possibly be a false claim.")

    scraped_content = (
    f"{scraped_df['Title'][0]} \n{scraped_df['Content'][0]}")
    
    # Perform text classification [source,claim]
    ans = pipe([[[scraped_content,request.input]]], truncation=True, padding='max_length')

    # Display the result
    if ans[0]['label'] == 'INCORRECT':
        ans[0]['score'] = 1 - ans[0]['score']
    
    classification_result = {
        "label": ans[0]['label'],
        "score": ans[0]['score']
    }
    return {'article':scraped_content,'result':classification_result}

#-------------------------------------------------------------------------------------------------------------------------------------
tokenizer_Bert_Binary_FakeReal = AutoTokenizer.from_pretrained(
    "Arjun24420/BERT-FakeOrReal-BinaryClassification")
model_Bert_Binary_FakeReal = AutoModelForSequenceClassification.from_pretrained(
    "Arjun24420/BERT-FakeOrReal-BinaryClassification")

# Define class labels mapping
class_mapping_Bert_Binary_FakeReal = {
    1: 'Reliable',
    0: 'Unreliable',
}

@app.post("/BertBinaryfr")
def predict_Bert_Binary_FakeorReal(request: inputRequest):
    # Tokenize the input text
    inputs = tokenizer_Bert_Binary_FakeReal(request.input, padding=True, truncation=True,
                       max_length=512, return_tensors="pt")

    # Get model output (logits)
    outputs = model_Bert_Binary_FakeReal(**inputs)

    # Calculate probabilities
    probs = outputs.logits.softmax(1)

    # Get the probabilities for each class
    class_probabilities = {class_mapping_Bert_Binary_FakeReal[i]: probs[0, i].item()
                           for i in range(probs.shape[1])}

    return class_probabilities
#-------------------------------------------------------------------------------------------------------------------------------------