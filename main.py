from fastapi import Depends, FastAPI, HTTPException, status, Header, Body
from pydantic import BaseModel
from concurrent.futures import ThreadPoolExecutor
from fastapi.middleware.cors import CORSMiddleware
from pymongo import MongoClient

from schemas import inputRequest

from transformers import pipeline
import google.generativeai as genai
from scrapingbsf import make_data

import requests
from bs4 import BeautifulSoup

app = FastAPI(title="IPD Back-End",)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

conn = MongoClient(
    "mongodb+srv://Aj_3001:RacWMGQ6k9gLKP0g@cluster0.vflcwlu.mongodb.net")
db = conn.course_rec.course_rec

genai.configure(api_key="AIzaSyAYwK3xLs3CoOevA29JgDUuMCx_rGQQIgA")
gemini_model = genai.GenerativeModel('gemini-pro')

@app.get("/")
def read_root():
    return {"You're not": "supposed to be here"}

#To answer the question generated by gemini via google
def google_search(query):
    headers = {
        'User-agent':
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'
    }
    
    # Perform the Google search
    search_url = f'https://www.google.com/search?q={query}'
    html = requests.get(search_url, headers=headers)
    
    # Parse the HTML response
    soup = BeautifulSoup(html.text, 'html.parser')
    
    # Extract the answer (assuming it's in a specific class)
    if soup.select_one('.DI6Ufb'):
        answer = soup.select_one('.DI6Ufb').text
    else:
        answer = 'No Quick Answer'
    
    return answer

@app.post("/gemini")
async def gemini(request: inputRequest):
    
    response = gemini_model.generate_content(
        f'''You're tasked with fact-checking news headlines for accuracy. Given a headline, generate 1 questions that needs to be true to verify the headlines authenticity using a Google search to scrape the answer from the quick search box. Ask the crucial questions first.
            Your output should only be in the form of a list data structure for me to ingest in my backend without any other text -> [Question1] with the actual question in place of Question1.
            The headline is : {request.input}'''
    )
    quickSearchAnswer = google_search(response.text[0])
    return {"Question": response.text,'Answer':quickSearchAnswer}

#To scrape headlines for the FactCC endpoint
def dataframegen(text_input):
    scraped_df = make_data(text_input)
    scraped_df.dropna(inplace=True)
    return scraped_df

pipe = pipeline(model="manueldeprada/FactCC", task="text-classification", max_length=512)

@app.post("/FactCC")
async def factCC(request: inputRequest):
    scraped_df =  dataframegen(request.input)

    #To make sure best scraped article picked
    scraped_df = scraped_df.sort_values(by='Content', key=lambda x: x.str.len(), ascending=False) 
    if len(scraped_df) == 0 or not scraped_df['Title'][0]:
        raise HTTPException(status_code=404, detail="Could not retrieve articles related to headlines")

    scraped_content = (
    f"{scraped_df['Title'][0]} \n{scraped_df['Content'][0]}")

    # after 400 tokens slice the scraped_content till the next fullstop
    # if len(scraped_content) > 400:
    #     scraped_content = scraped_content[:400] + scraped_content[400:].split('.')[0] + '.'
    
    # Perform text classification [source,claim]
    ans = pipe([[[scraped_content,request.input]]], truncation=True, padding='max_length')

    # Display the result
    if ans[0]['label'] == 'INCORRECT':
        ans[0]['score'] = 1 - ans[0]['score']
    
    classification_result = {
        "label": ans[0]['label'],
        "score": ans[0]['score']
    }
    return {'article':scraped_content,'result':classification_result}

@app.post("/article")
async def test(request: inputRequest):
    scraped_df =  dataframegen(request.input)
    scraped_df = scraped_df.sort_values(by='Content', key=lambda x: x.str.len(), ascending=False)
    scraped_content = (
    f"{scraped_df['Title'][0]} \n{scraped_df['Content'][0]}")

    return {'articles':scraped_df}
