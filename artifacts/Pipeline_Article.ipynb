{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,pipeline\n",
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from random import uniform, choice\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from googlesearch import search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Test_dataset(FINAL).csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_links(query, num_links=1):\n",
    "    linksgot = []\n",
    "    try:\n",
    "        # Perform Google search and get the top links\n",
    "        search_results = search(query, num_results=num_links)\n",
    "\n",
    "        # Print the top links\n",
    "        for i, link in enumerate(search_results, start=1):\n",
    "            linksgot.append(link)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return linksgot\n",
    "\n",
    "\n",
    "def get_title_and_content(search_query_results):\n",
    "    article_titles = []\n",
    "    article_content = []\n",
    "    if search_query_results:\n",
    "        for results in search_query_results:\n",
    "            try:\n",
    "                # Send a request to the URL and get the HTML content\n",
    "                response = requests.get(results)\n",
    "                soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "                currentp = \"\"\n",
    "                # Scrape <p> tags\n",
    "                p_tags = soup.find_all('p')\n",
    "                for p in p_tags:\n",
    "                    currentp += p.text\n",
    "                article_content.append(currentp)\n",
    "\n",
    "                currenth1 = \"\"\n",
    "                # Scrape <h1> tags\n",
    "                h1_tags = soup.find_all('h1')\n",
    "                for h1 in h1_tags:\n",
    "                    currenth1 += h1.text\n",
    "\n",
    "                article_titles.append(currenth1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "    return article_titles, article_content\n",
    "\n",
    "\n",
    "# Get the titles and contents\n",
    "def make_data(search_query_results):\n",
    "    titles, contents = get_title_and_content(\n",
    "        get_top_links(search_query_results))\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    data = {'Title': titles, 'Content': contents}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To scrape headlines for the FactCC endpoint\n",
    "def dataframegen(text_input):\n",
    "    scraped_df = make_data(text_input)\n",
    "    scraped_df.dropna(inplace=True)\n",
    "    return scraped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load FactCC Model pipeline\n",
    "pipe = pipeline(model=\"manueldeprada/FactCC\", task=\"text-classification\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factCC(input_headline : str):\n",
    "    scraped_df =  dataframegen(input_headline)\n",
    "\n",
    "    # Convert the 'Content' column to strings\n",
    "    scraped_df['Content'] = scraped_df['Content'].astype(str)\n",
    "\n",
    "    # Sort the DataFrame based on the length of the strings in the 'Content' column\n",
    "    scraped_df = scraped_df.sort_values(by='Content', key=lambda x: x.str.len(), ascending=False)\n",
    "\n",
    "    # print(f\"Title: {scraped_df['Content'][0][:100]}\")\n",
    "\n",
    "    if len(scraped_df) == 0 or not scraped_df['Content'][0] or '403 Forbidden' in scraped_df['Content'][0] or '403 Forbidden' in scraped_df['Title'][0] :\n",
    "        #Could not retrieve articles related to headline, Could possibly be a false claim OR Scraper got blocked/forbidden\n",
    "        return False\n",
    "\n",
    "    scraped_content = (\n",
    "    f\"{scraped_df['Title'][0]} \\n{scraped_df['Content'][0]}\")\n",
    "    \n",
    "    # Perform text classification [source,claim]\n",
    "    ans = pipe([[[scraped_content,input_headline]]], truncation=True, padding='max_length')\n",
    "\n",
    "    if ans[0]['label'] == 'CORRECT':\n",
    "        return True, scraped_content\n",
    "    else:\n",
    "        return False, scraped_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_decision = factCC(df['Headline'][0])\n",
    "actual_decision = df['Label'][0]\n",
    "print(model_decision,actual_decision)\n",
    "if model_decision == actual_decision:\n",
    "    print('Correct Decision')\n",
    "else:\n",
    "    print('Incorrect Decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming df and factCC are already defined\n",
    "\n",
    "predictions = []\n",
    "actuals = []\n",
    "results = []\n",
    "batch_size = 10\n",
    "\n",
    "csv_filename = 'article_pipeline.csv'\n",
    "\n",
    "# Delete the file if it already exists\n",
    "if os.path.exists(csv_filename):\n",
    "    os.remove(csv_filename)\n",
    "    print(f\"Existing {csv_filename} has been deleted.\")\n",
    "\n",
    "try:\n",
    "    # Open the CSV file for writing\n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Index', 'Headline','Scraped Content',  'Model Decision', 'Actual Decision'])\n",
    "        \n",
    "        # Iterate over each row in the dataframe with tqdm for progress\n",
    "        for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            try:\n",
    "                model_decision, scraped_content = factCC(row['Headline'])\n",
    "                actual_decision = row['Label']\n",
    "                predictions.append(model_decision)\n",
    "                actuals.append(actual_decision)\n",
    "                \n",
    "                results.append([index, row['Headline'],scraped_content,model_decision, actual_decision])\n",
    "                time.sleep(2)\n",
    "                # Write to CSV in batches of 10\n",
    "                if len(results) % batch_size == 0:\n",
    "                    # print(f\"Writing batch of {batch_size} to CSV...\")\n",
    "                    csvwriter.writerows(results)\n",
    "                    csvfile.flush()  # Force write to disk\n",
    "                    os.fsync(csvfile.fileno())  # Ensure it's written to disk\n",
    "                    # print(f\"Batch written. Current file size: {os.path.getsize(csv_filename)} bytes\")\n",
    "                    results = []\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {index}: {str(e)}\")\n",
    "        \n",
    "        # Write any remaining results\n",
    "        if results:\n",
    "            # print(f\"Writing final batch of {len(results)} to CSV...\")\n",
    "            csvwriter.writerows(results)\n",
    "            csvfile.flush()\n",
    "            os.fsync(csvfile.fileno())\n",
    "            # print(f\"Final batch written. Current file size: {os.path.getsize(csv_filename)} bytes\")\n",
    "\n",
    "    # Calculate the metrics\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    precision = precision_score(actuals, predictions, pos_label=True)\n",
    "    recall = recall_score(actuals, predictions, pos_label=True)\n",
    "    f1 = f1_score(actuals, predictions, pos_label=True)\n",
    "\n",
    "    # Append the accuracy to the CSV file\n",
    "    with open(csv_filename, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow([])  # Add an empty row for separation\n",
    "        csvwriter.writerow(['Metric', 'Value'])\n",
    "        csvwriter.writerow(['Accuracy', accuracy])\n",
    "        csvwriter.writerow(['Precision', precision])\n",
    "        csvwriter.writerow(['Recall', recall])\n",
    "        csvwriter.writerow(['F1 Score', f1])\n",
    "\n",
    "    print(f'Results written to {csv_filename}')\n",
    "    print(f'Final file size: {os.path.getsize(csv_filename)} bytes')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    if os.path.exists(csv_filename):\n",
    "        print(f\"CSV file exists. Size: {os.path.getsize(csv_filename)} bytes\")\n",
    "    else:\n",
    "        print(\"CSV file does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
