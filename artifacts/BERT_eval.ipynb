{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_set = pd.read_csv('Test_dataset(FINAL).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Source</th>\n",
       "      <th>Question_phi</th>\n",
       "      <th>Question_Mistral</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NASA’s Perseverance rover finds its first poss...</td>\n",
       "      <td>sciencenews.org</td>\n",
       "      <td>\"Has NASA's Perseverance rover discovered evid...</td>\n",
       "      <td>\"Has NASA officially announced the discovery o...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sepsis tests take days  putting patients at ri...</td>\n",
       "      <td>sciencenews.org</td>\n",
       "      <td>\"What is the current average wait time for sep...</td>\n",
       "      <td>\"Is there a recent study or research that show...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nasa's DART asteroid unlocks complex history o...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/</td>\n",
       "      <td>\"What is the history of NASA's DART mission an...</td>\n",
       "      <td>\"Has NASA's DART mission provided evidence of ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Say goodbye to back pain  patients go for adva...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/</td>\n",
       "      <td>\"What are the benefits of advanced endoscopy s...</td>\n",
       "      <td>\"Has 'advanced endoscopy spine surgery for sci...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neurodivergent children more likely to develop...</td>\n",
       "      <td>https://timesofindia.indiatimes.com/</td>\n",
       "      <td>\"What does the study find about the likelihood...</td>\n",
       "      <td>\"Is there a peer-reviewed study titled 'Neurod...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  NASA’s Perseverance rover finds its first poss...   \n",
       "1  Sepsis tests take days  putting patients at ri...   \n",
       "2  Nasa's DART asteroid unlocks complex history o...   \n",
       "3  Say goodbye to back pain  patients go for adva...   \n",
       "4  Neurodivergent children more likely to develop...   \n",
       "\n",
       "                                 Source  \\\n",
       "0                       sciencenews.org   \n",
       "1                       sciencenews.org   \n",
       "2  https://timesofindia.indiatimes.com/   \n",
       "3  https://timesofindia.indiatimes.com/   \n",
       "4  https://timesofindia.indiatimes.com/   \n",
       "\n",
       "                                        Question_phi  \\\n",
       "0  \"Has NASA's Perseverance rover discovered evid...   \n",
       "1  \"What is the current average wait time for sep...   \n",
       "2  \"What is the history of NASA's DART mission an...   \n",
       "3  \"What are the benefits of advanced endoscopy s...   \n",
       "4  \"What does the study find about the likelihood...   \n",
       "\n",
       "                                    Question_Mistral  Label  \n",
       "0  \"Has NASA officially announced the discovery o...   True  \n",
       "1  \"Is there a recent study or research that show...   True  \n",
       "2  \"Has NASA's DART mission provided evidence of ...   True  \n",
       "3  \"Has 'advanced endoscopy spine surgery for sci...   True  \n",
       "4  \"Is there a peer-reviewed study titled 'Neurod...   True  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in df['label'] are boolean (True/False).\n"
     ]
    }
   ],
   "source": [
    "# Check if all values in df_test_set['Label'] are boolean\n",
    "all_boolean = df_test_set['Label'].apply(lambda x: isinstance(x, bool)).all()\n",
    "\n",
    "if all_boolean:\n",
    "    print(\"All values in df['label'] are boolean (True/False).\")\n",
    "else:\n",
    "    print(\"Not all values in df['label'] are boolean. There may be other data types present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face Link - https://huggingface.co/Arjun24420/BERT-FakeNews-BinaryClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_Bert_Binary_FakeReal = AutoTokenizer.from_pretrained(\n",
    "    \"Arjun24420/BERT-FakeNews-BinaryClassification\")\n",
    "model_Bert_Binary_FakeReal = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"Arjun24420/BERT-FakeNews-BinaryClassification\")\n",
    "\n",
    "# Define class labels mapping\n",
    "class_mapping_Bert_Binary_FakeReal = {\n",
    "    1: 'Reliable',\n",
    "    0: 'Unreliable',\n",
    "}\n",
    "\n",
    "def predict_Bert_Binary_FakeOrReal(input_headline):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer_Bert_Binary_FakeReal(input_headline, padding=True, truncation=True,\n",
    "                       max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Get model output (logits)\n",
    "    outputs = model_Bert_Binary_FakeReal(**inputs)\n",
    "\n",
    "    # Calculate probabilities\n",
    "    probs = outputs.logits.softmax(1)\n",
    "\n",
    "    # Get the probabilities for each class\n",
    "    class_probabilities = {class_mapping_Bert_Binary_FakeReal[i]: probs[0, i].item()\n",
    "                           for i in range(probs.shape[1])}\n",
    "\n",
    "    return class_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unreliable': 0.5826311707496643, 'Reliable': 0.4173688292503357}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example Usage\n",
    "predict_Bert_Binary_FakeOrReal(\"Paris 2024 Olympics: Leon Marchand fails to achieve any medals in the competition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 688/688 [00:30<00:00, 22.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.5334\n",
      "Precision: 0.52\n",
      "Recall: 0.71\n",
      "F1-Score: 0.60\n",
      "\n",
      "Confusion Matrix:\n",
      "[[124 220]\n",
      " [101 243]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_bert_model(df):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(df)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=total_predictions, desc=\"Evaluating\"):\n",
    "        headline = row['Headline']\n",
    "        true_label = row['Label']\n",
    "        y_true.append(true_label)\n",
    "\n",
    "        # Get prediction from BERT model\n",
    "        bert_proba = predict_Bert_Binary_FakeOrReal(headline)\n",
    "\n",
    "        # Determine label\n",
    "        predicted_label = bert_proba['Reliable'] > bert_proba['Unreliable']\n",
    "        y_pred.append(predicted_label)\n",
    "\n",
    "        # Compare prediction with true label\n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy, y_true, y_pred\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, y_test_set, y_test_set_pred = evaluate_bert_model(df_test_set)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(y_test_set, y_test_set_pred)\n",
    "recall = recall_score(y_test_set, y_test_set_pred)\n",
    "f1 = f1_score(y_test_set, y_test_set_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Create and display confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_set, y_test_set_pred)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
